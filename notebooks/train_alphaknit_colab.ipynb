{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß∂ AlphaKnit ‚Äî Train KnittingTransformer on Colab\n",
                "\n",
                "**AlphaKnit v6.6-F: Scientific Falsification & Discovery**\n",
                "\n",
                "Notebook n√†y gi√∫p b·∫°n train m√¥ h√¨nh `KnittingTransformer` tr√™n Google Colab v·ªõi GPU T4/A100.\n",
                "\n",
                "**Pipeline:** Point Cloud (`.npy`) ‚Üí Encoder (PointNet) ‚Üí Transformer Decoder ‚Üí Edge-Action Sequence (stitch tokens)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. üîß Setup & Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ki·ªÉm tra GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive (ƒë·ªÉ l∆∞u checkpoints & dataset)\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo t·ª´ GitHub (thay URL n·∫øu c·∫ßn)\n",
                "# N·∫øu repo private, d√πng: git clone https://<TOKEN>@github.com/user/AlphaKnit.git\n",
                "\n",
                "import os\n",
                "\n",
                "REPO_DIR = \"/content/AlphaKnit\"\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    # === OPTION A: Clone t·ª´ GitHub ===\n",
                "    # !git clone https://github.com/<your-username>/AlphaKnit.git {REPO_DIR}\n",
                "\n",
                "    # === OPTION B: Copy t·ª´ Google Drive ===\n",
                "    !cp -r \"/content/drive/MyDrive/AlphaKnit\" {REPO_DIR}\n",
                "\n",
                "    print(f\"‚úÖ Project ready at {REPO_DIR}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Project already exists at {REPO_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√†i dependencies\n",
                "!pip install -q scipy trimesh networkx webdataset tqdm matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Th√™m project v√†o Python path\n",
                "import sys\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, \"src\"))\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, \"scripts\"))\n",
                "\n",
                "# Verify import\n",
                "from alphaknit import config\n",
                "from alphaknit.model import KnittingTransformer\n",
                "from alphaknit.knitting_dataset import KnittingDataset, make_dataloaders\n",
                "\n",
                "print(f\"‚úÖ AlphaKnit imported successfully\")\n",
                "print(f\"   Vocab size: {config.VOCAB_SIZE}\")\n",
                "print(f\"   D_MODEL: {config.D_MODEL}, N_HEADS: {config.N_HEADS}, N_LAYERS: {config.N_LAYERS}\")\n",
                "print(f\"   MAX_SEQ_LEN: {config.MAX_SEQ_LEN}, N_POINTS: {config.N_POINTS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. üèóÔ∏è Generate Dataset (Optional)\n",
                "\n",
                "N·∫øu b·∫°n ch∆∞a c√≥ dataset, ch·∫°y cell d∆∞·ªõi ƒë√¢y ƒë·ªÉ t·∫°o dataset tr·ª±c ti·∫øp th√†nh **WebDataset shards** tr√™n Google Drive.\n",
                "\n",
                "Pipeline: `SpatialGeneratorV2` ‚Üí `GraphValidator` ‚Üí `ForwardSimulator` ‚Üí PCA Align ‚Üí Tensorize ‚Üí Pack `.tar` shards\n",
                "\n",
                "**∆Øu ƒëi·ªÉm WebDataset shards:**\n",
                "- Tr√°nh gi·ªõi h·∫°n 100k files c·ªßa Google Drive FUSE\n",
                "- I/O streaming nhanh h∆°n nhi·ªÅu so v·ªõi ƒë·ªçc t·ª´ng file\n",
                "- Shuffle entropy t·ªët h∆°n\n",
                "\n",
                "> ‚ö†Ô∏è **Th·ªùi gian ∆∞·ªõc t√≠nh:** ~5k samples/10 ph√∫t tr√™n Colab CPU. 50k samples ‚âà 1.5-2 gi·ªù."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# C·∫§U H√åNH DATASET GENERATION\n",
                "# ============================================================\n",
                "N_SAMPLES  = 50000   # T·ªïng s·ªë samples c·∫ßn t·∫°o\n",
                "SHARD_SIZE = 1000    # S·ªë samples m·ªói shard (.tar file)\n",
                "\n",
                "# Output tr√™n Google Drive (persistent, kh√¥ng m·∫•t khi session timeout)\n",
                "SHARDS_OUTPUT_DIR = \"/content/drive/MyDrive/AlphaKnit/data/processed/shards_phase9b_full\"\n",
                "\n",
                "print(f\"Will generate {N_SAMPLES} samples into {N_SAMPLES // SHARD_SIZE} shards\")\n",
                "print(f\"Output: {SHARDS_OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import tarfile\n",
                "import tempfile\n",
                "import numpy as np\n",
                "import torch\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "from alphaknit.dataset_builder import DatasetBuilder\n",
                "from alphaknit import config\n",
                "from pack_tensor_dataset import build_tensor_sample\n",
                "\n",
                "os.makedirs(SHARDS_OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Check for existing shards ‚Äî cho ph√©p resume generation\n",
                "existing_shards = sorted([f for f in os.listdir(SHARDS_OUTPUT_DIR) if f.endswith('.tar')])\n",
                "if existing_shards:\n",
                "    print(f\"‚ö†Ô∏è Found {len(existing_shards)} existing shards in output dir.\")\n",
                "    print(f\"   Last shard: {existing_shards[-1]}\")\n",
                "    resume_from = len(existing_shards)\n",
                "    samples_already_done = resume_from * SHARD_SIZE\n",
                "    print(f\"   Resuming from shard {resume_from} (‚âà{samples_already_done} samples done)\")\n",
                "else:\n",
                "    resume_from = 0\n",
                "    samples_already_done = 0\n",
                "\n",
                "remaining = N_SAMPLES - samples_already_done\n",
                "if remaining <= 0:\n",
                "    print(f\"‚úÖ Already have enough shards ({samples_already_done} ‚â• {N_SAMPLES})\")\n",
                "else:\n",
                "    print(f\"\\nüöÄ Generating {remaining} samples...\")\n",
                "    builder = DatasetBuilder(output_dir=\"/tmp/_alphaknit_gen_dummy\")\n",
                "    temp_dir = tempfile.mkdtemp()\n",
                "\n",
                "    skipped = 0\n",
                "    samples_generated = 0\n",
                "    shard_id = resume_from\n",
                "\n",
                "    pbar = tqdm(total=remaining, desc=\"Generating samples\")\n",
                "\n",
                "    while samples_generated < remaining:\n",
                "        shard_path = os.path.join(SHARDS_OUTPUT_DIR, f\"shard-{shard_id:04d}.tar\")\n",
                "        samples_in_this_shard = min(SHARD_SIZE, remaining - samples_generated)\n",
                "\n",
                "        with tarfile.open(shard_path, \"w\") as tar:\n",
                "            count_in_shard = 0\n",
                "            while count_in_shard < samples_in_this_shard:\n",
                "                global_idx = samples_already_done + samples_generated + count_in_shard\n",
                "                raw_sample = builder._generate_one(global_idx)\n",
                "\n",
                "                if raw_sample is None:\n",
                "                    skipped += 1\n",
                "                    if skipped > N_SAMPLES * 5:\n",
                "                        print(f\"‚ö†Ô∏è Too many skipped samples ({skipped}). Stopping.\")\n",
                "                        break\n",
                "                    continue\n",
                "\n",
                "                name = raw_sample['id']\n",
                "                pc = raw_sample.pop(\"point_cloud\")\n",
                "\n",
                "                # T·∫°m l∆∞u JSON + NPY ƒë·ªÉ ƒëi qua tensorizer\n",
                "                tmp_json = os.path.join(temp_dir, f\"{name}.json\")\n",
                "                tmp_npy  = os.path.join(temp_dir, f\"{name}.npy\")\n",
                "\n",
                "                with open(tmp_json, \"w\") as f:\n",
                "                    json.dump(raw_sample, f)\n",
                "                np.save(tmp_npy, pc)\n",
                "\n",
                "                # Tensorize: pad point cloud + build src/tgt token sequences\n",
                "                tensor_sample = build_tensor_sample(\n",
                "                    tmp_json, tmp_npy,\n",
                "                    config.MAX_SEQ_LEN, config.N_POINTS\n",
                "                )\n",
                "\n",
                "                # Pack v√†o tar shard\n",
                "                tmp_pt = os.path.join(temp_dir, f\"{name}.pt\")\n",
                "                torch.save(tensor_sample, tmp_pt)\n",
                "                tar.add(tmp_pt, arcname=f\"{name}.pt\")\n",
                "\n",
                "                # D·ªçn tmp\n",
                "                os.remove(tmp_json)\n",
                "                os.remove(tmp_npy)\n",
                "                os.remove(tmp_pt)\n",
                "\n",
                "                count_in_shard += 1\n",
                "                samples_generated += 1\n",
                "                pbar.update(1)\n",
                "\n",
                "        shard_id += 1\n",
                "\n",
                "    pbar.close()\n",
                "    os.rmdir(temp_dir)\n",
                "\n",
                "    total_shards = len([f for f in os.listdir(SHARDS_OUTPUT_DIR) if f.endswith('.tar')])\n",
                "    print(f\"\\nüéâ Done! Total: {total_shards} shards, {samples_already_done + samples_generated} samples\")\n",
                "    print(f\"   Skipped (invalid): {skipped}\")\n",
                "    print(f\"   Saved to: {SHARDS_OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify shards\n",
                "shard_files = sorted([f for f in os.listdir(SHARDS_OUTPUT_DIR) if f.endswith('.tar')])\n",
                "total_size_mb = sum(os.path.getsize(os.path.join(SHARDS_OUTPUT_DIR, f)) for f in shard_files) / 1e6\n",
                "\n",
                "print(f\"üì¶ {len(shard_files)} shards | {total_size_mb:.0f} MB total\")\n",
                "for f in shard_files[:5]:\n",
                "    size = os.path.getsize(os.path.join(SHARDS_OUTPUT_DIR, f)) / 1e6\n",
                "    print(f\"   {f} ({size:.1f} MB)\")\n",
                "if len(shard_files) > 5:\n",
                "    print(f\"   ... ({len(shard_files) - 5} more shards)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. üì¶ Dataset Configuration\n",
                "\n",
                "Dataset g·ªìm WebDataset `.tar` shards, m·ªói shard ch·ª©a `sample_XXXXX.pt` ƒë√£ tensorize s·∫µn:\n",
                "- `pc`: Point cloud `(N_POINTS, 3)` float32\n",
                "- `src`: Teacher forcing input `(MAX_SEQ_LEN, 3)` long ‚Äî `<SOS> + edge_tuples`\n",
                "- `tgt`: Prediction target `(MAX_SEQ_LEN, 3)` long ‚Äî `edge_tuples + <EOS>`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# C·∫§U H√åNH DATASET PATH\n",
                "# ============================================================\n",
                "\n",
                "# WebDataset shards tr√™n Google Drive\n",
                "DATASET_DIR = \"/content/drive/MyDrive/AlphaKnit/data/processed/shards_phase9b_full/shard-{0000..0049}.tar\"\n",
                "\n",
                "# N·∫øu mu·ªën d√πng Map-Style dataset (folder ch·ª©a .json + .npy):\n",
                "# DATASET_DIR = \"/content/drive/MyDrive/AlphaKnit/data/processed/dataset\"\n",
                "\n",
                "print(f\"Dataset: {DATASET_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview 1 sample t·ª´ shard\n",
                "import tarfile\n",
                "import io\n",
                "\n",
                "if '.tar' in DATASET_DIR:\n",
                "    # WebDataset: ƒë·ªçc tr·ª±c ti·∫øp t·ª´ tar\n",
                "    base_dir = DATASET_DIR.split('{')[0]\n",
                "    first_shard = base_dir + \"0000.tar\"\n",
                "    if os.path.exists(first_shard):\n",
                "        with tarfile.open(first_shard, 'r') as tar:\n",
                "            member = tar.getmembers()[0]\n",
                "            f = tar.extractfile(member)\n",
                "            sample = torch.load(io.BytesIO(f.read()), map_location='cpu', weights_only=False)\n",
                "        \n",
                "        print(f\"Sample from {os.path.basename(first_shard)} ‚Üí {member.name}\")\n",
                "        print(f\"  pc shape:  {sample['pc'].shape}  dtype: {sample['pc'].dtype}\")\n",
                "        print(f\"  src shape: {sample['src'].shape}  dtype: {sample['src'].dtype}\")\n",
                "        print(f\"  tgt shape: {sample['tgt'].shape}  dtype: {sample['tgt'].dtype}\")\n",
                "        \n",
                "        # Decode first few tokens\n",
                "        print(f\"\\n  First 5 src tuples (type, p1, p2):\")\n",
                "        for i in range(min(5, sample['src'].shape[0])):\n",
                "            t, p1, p2 = sample['src'][i].tolist()\n",
                "            name = config.ID_TO_TOKEN.get(t, f'<ID:{t}>')\n",
                "            print(f\"    [{i}] {name:8s} (p1={p1}, p2={p2})\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è First shard not found: {first_shard}\")\n",
                "        print(f\"   Available files: {os.listdir(os.path.dirname(first_shard))[:5]}\")\n",
                "else:\n",
                "    # Map-Style: ƒë·ªçc tr·ª±c ti·∫øp json + npy\n",
                "    import json\n",
                "    import numpy as np\n",
                "    if os.path.isdir(DATASET_DIR):\n",
                "        sample_files = sorted([f for f in os.listdir(DATASET_DIR) if f.endswith('.json')])\n",
                "        if sample_files:\n",
                "            sid = sample_files[0].replace('.json', '')\n",
                "            with open(os.path.join(DATASET_DIR, f\"{sid}.json\")) as f:\n",
                "                meta = json.load(f)\n",
                "            pc = np.load(os.path.join(DATASET_DIR, f\"{sid}.npy\"))\n",
                "            print(f\"Sample: {sid}\")\n",
                "            print(f\"  Point cloud: {pc.shape}, range [{pc.min():.3f}, {pc.max():.3f}]\")\n",
                "            edge_seq = meta.get('edge_sequence', [])\n",
                "            print(f\"  Edge sequence: {len(edge_seq)} tuples\")\n",
                "            for t, p1, p2 in edge_seq[:5]:\n",
                "                print(f\"    {config.ID_TO_TOKEN.get(t, f'<ID:{t}>')}(p1={p1}, p2={p2})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. üöÄ Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# HYPERPARAMETERS ‚Äî T·ªëi ∆∞u cho Colab T4 (16GB VRAM)\n",
                "# ============================================================\n",
                "\n",
                "TRAIN_CONFIG = {\n",
                "    # Dataset\n",
                "    \"dataset_dir\": DATASET_DIR,\n",
                "\n",
                "    # Checkpoint ‚Äî L∆∞u l√™n Google Drive ƒë·ªÉ kh√¥ng m·∫•t khi session timeout\n",
                "    \"checkpoint_dir\": \"/content/drive/MyDrive/AlphaKnit/checkpoints\",\n",
                "    \"run_name\": \"colab_v6.6F\",\n",
                "\n",
                "    # Model architecture\n",
                "    \"d_model\": 128,\n",
                "    \"n_heads\": 4,\n",
                "    \"n_layers\": 3,\n",
                "    \"ffn_dim\": 256,\n",
                "\n",
                "    # Training\n",
                "    \"epochs\": 50,\n",
                "    \"batch_size\": 64,          # T4: 64 ok, A100: c√≥ th·ªÉ tƒÉng l√™n 128-256\n",
                "    \"lr\": 1e-3,\n",
                "    \"grad_accum_steps\": 2,     # Effective batch = 64 * 2 = 128\n",
                "    \"label_smoothing\": 0.1,\n",
                "    \"scheduler_type\": \"cosine\",\n",
                "\n",
                "    # Data loading\n",
                "    \"num_workers\": 2,\n",
                "    \"val_split\": 0.1,\n",
                "\n",
                "    # Phase transition\n",
                "    \"early_stop_patience\": 10,\n",
                "    \"log_compile_every\": 5,\n",
                "\n",
                "    # Device\n",
                "    \"device_str\": \"auto\",\n",
                "}\n",
                "\n",
                "# T·∫°o th∆∞ m·ª•c checkpoint\n",
                "os.makedirs(TRAIN_CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
                "print(\"‚úÖ Training config ready\")\n",
                "for k, v in TRAIN_CONFIG.items():\n",
                "    print(f\"   {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. üèãÔ∏è Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from alphaknit.train import train\n",
                "\n",
                "# ‚ñ∂ B·∫ÆT ƒê·∫¶U TRAINING\n",
                "history = train(**TRAIN_CONFIG)\n",
                "\n",
                "print(f\"\\nüéâ Training complete! {len(history)} epochs recorded.\")\n",
                "print(f\"üìÅ Checkpoints saved to: {TRAIN_CONFIG['checkpoint_dir']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. üîÑ Resume Training (n·∫øu b·ªã ng·∫Øt gi·ªØa ch·ª´ng)\n",
                "\n",
                "N·∫øu Colab session b·ªã timeout, ch·∫°y l·∫°i t·ª´ **Cell 1 (Setup)** r·ªìi nh·∫£y th·∫≥ng xu·ªëng ƒë√¢y."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from alphaknit.train import train\n",
                "\n",
                "RESUME_CONFIG = {\n",
                "    **TRAIN_CONFIG,\n",
                "    \"resume_auto\": True,       # T·ª± t√¨m checkpoint m·ªõi nh·∫•t\n",
                "    \"epochs\": 100,             # T·ªïng epochs mu·ªën ƒë·∫°t ƒë∆∞·ª£c\n",
                "    # \"force_phase2\": True,    # Uncomment n·∫øu mu·ªën force Physics phase\n",
                "    # \"reset_optimizer\": True, # Uncomment n·∫øu mu·ªën reset optimizer cho phase transition\n",
                "}\n",
                "\n",
                "history = train(**RESUME_CONFIG)\n",
                "\n",
                "print(f\"\\nüéâ Resumed training complete! {len(history)} epochs recorded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. üìä Training Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Load history t·ª´ file\n",
                "history_path = os.path.join(\n",
                "    TRAIN_CONFIG[\"checkpoint_dir\"],\n",
                "    f\"training_history_{TRAIN_CONFIG['run_name']}.json\"\n",
                ")\n",
                "\n",
                "if os.path.exists(history_path):\n",
                "    with open(history_path) as f:\n",
                "        history = json.load(f)\n",
                "    print(f\"‚úÖ Loaded {len(history)} epochs from history\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No history file found. Run training first.\")\n",
                "    history = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if history:\n",
                "    epochs_list = [h[\"epoch\"] for h in history]\n",
                "\n",
                "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "    fig.suptitle(\"AlphaKnit Training Dashboard\", fontsize=16, fontweight=\"bold\")\n",
                "\n",
                "    # 1. Loss curves\n",
                "    ax = axes[0, 0]\n",
                "    ax.plot(epochs_list, [h[\"train_loss\"] for h in history], label=\"Train\", color=\"#2196F3\")\n",
                "    ax.plot(epochs_list, [h[\"val_loss\"] for h in history], label=\"Val\", color=\"#F44336\", ls=\"--\")\n",
                "    ax.set_title(\"Loss\"); ax.set_xlabel(\"Epoch\"); ax.legend(); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    # 2. Entropy\n",
                "    ax = axes[0, 1]\n",
                "    ax.plot(epochs_list, [h.get(\"train_entropy\", 0) for h in history], color=\"#9C27B0\")\n",
                "    ax.set_title(\"Token Entropy\"); ax.set_xlabel(\"Epoch\"); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    # 3. Structural Accuracy\n",
                "    ax = axes[0, 2]\n",
                "    ax.plot(epochs_list, [h.get(\"struct_acc\", 0) for h in history], color=\"#4CAF50\")\n",
                "    ax.set_title(\"Structural Top-1 Acc\"); ax.set_xlabel(\"Epoch\"); ax.set_ylim(0, 1); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    # 4. Compile Success Rate\n",
                "    ax = axes[1, 0]\n",
                "    ce = [h[\"epoch\"] for h in history if \"compile_success_rate\" in h]\n",
                "    cr = [h[\"compile_success_rate\"] for h in history if \"compile_success_rate\" in h]\n",
                "    if ce: ax.plot(ce, cr, 'o-', color=\"#FF9800\", ms=6)\n",
                "    ax.set_title(\"Compile Success Rate\"); ax.set_xlabel(\"Epoch\"); ax.set_ylim(0, 1); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    # 5. Phase Lag\n",
                "    ax = axes[1, 1]\n",
                "    ax.plot(epochs_list, [h.get(\"phase_lag\", 0) for h in history], color=\"#00BCD4\")\n",
                "    ax.set_title(\"Phase Lag\"); ax.set_xlabel(\"Epoch\"); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    # 6. PDI & Tension\n",
                "    ax = axes[1, 2]\n",
                "    ax.plot(epochs_list, [h.get(\"train_pdi\", 0) for h in history], label=\"PDI\", color=\"#E91E63\")\n",
                "    ax.plot(epochs_list, [h.get(\"train_tension\", 0) for h in history], label=\"Tension\", color=\"#795548\")\n",
                "    ax.set_title(\"PDI & Tension\"); ax.set_xlabel(\"Epoch\"); ax.legend(); ax.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(TRAIN_CONFIG[\"checkpoint_dir\"], \"training_dashboard.png\"), dpi=150)\n",
                "    plt.show()\n",
                "    print(\"üìä Dashboard saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. üíæ Export Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ki·ªÉm tra c√°c checkpoints\n",
                "ckpt_dir = TRAIN_CONFIG[\"checkpoint_dir\"]\n",
                "ckpt_files = sorted([f for f in os.listdir(ckpt_dir) if f.endswith('.pt')])\n",
                "\n",
                "print(f\"üìÅ Checkpoints in {ckpt_dir}:\")\n",
                "for f in ckpt_files:\n",
                "    size_mb = os.path.getsize(os.path.join(ckpt_dir, f)) / 1e6\n",
                "    print(f\"   {f} ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download best model v·ªÅ m√°y local\n",
                "from google.colab import files\n",
                "\n",
                "best_model_path = os.path.join(ckpt_dir, f\"best_model_{TRAIN_CONFIG['run_name']}.pt\")\n",
                "if os.path.exists(best_model_path):\n",
                "    ckpt = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n",
                "    print(f\"Best model ‚Äî Epoch: {ckpt['epoch']}, Val Loss: {ckpt['val_loss']:.4f}\")\n",
                "    files.download(best_model_path)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Best model not found. Train first!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. üî¨ Quick Inference Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model v√† test inference\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "model = KnittingTransformer(\n",
                "    d_model=TRAIN_CONFIG[\"d_model\"],\n",
                "    n_heads=TRAIN_CONFIG[\"n_heads\"],\n",
                "    n_layers=TRAIN_CONFIG[\"n_layers\"],\n",
                "    ffn_dim=TRAIN_CONFIG[\"ffn_dim\"],\n",
                ").to(device)\n",
                "\n",
                "best_model_path = os.path.join(ckpt_dir, f\"best_model_{TRAIN_CONFIG['run_name']}.pt\")\n",
                "if os.path.exists(best_model_path):\n",
                "    ckpt = torch.load(best_model_path, map_location=device, weights_only=True)\n",
                "    model.load_state_dict(ckpt[\"model_state\"])\n",
                "    model.eval()\n",
                "    print(f\"‚úÖ Model loaded (Epoch {ckpt['epoch']}, Val Loss {ckpt['val_loss']:.4f})\")\n",
                "\n",
                "    # Load 1 sample t·ª´ shard ƒë·ªÉ test\n",
                "    import tarfile, io\n",
                "    base_dir = DATASET_DIR.split('{')[0] if '{' in DATASET_DIR else DATASET_DIR + '/'\n",
                "    first_shard = base_dir + \"0000.tar\" if '.tar' in DATASET_DIR else None\n",
                "\n",
                "    if first_shard and os.path.exists(first_shard):\n",
                "        with tarfile.open(first_shard, 'r') as tar:\n",
                "            member = tar.getmembers()[0]\n",
                "            f = tar.extractfile(member)\n",
                "            sample = torch.load(io.BytesIO(f.read()), map_location='cpu', weights_only=False)\n",
                "        pc = sample['pc'].unsqueeze(0).to(device)\n",
                "    elif os.path.isdir(DATASET_DIR):\n",
                "        dataset = KnittingDataset(DATASET_DIR)\n",
                "        sample = dataset[0]\n",
                "        pc = sample['point_cloud'].unsqueeze(0).to(device)\n",
                "    else:\n",
                "        pc = None\n",
                "        print(\"‚ö†Ô∏è No dataset found for inference test.\")\n",
                "\n",
                "    if pc is not None:\n",
                "        with torch.no_grad():\n",
                "            pred_tuples = model.greedy_decode(pc, max_len=config.MAX_SEQ_LEN)\n",
                "\n",
                "        pred = pred_tuples[0]\n",
                "        print(f\"\\nüìã Generated sequence ({len(pred)} tuples):\")\n",
                "        for i, (t, p1, p2) in enumerate(pred[:20]):\n",
                "            token_name = config.ID_TO_TOKEN.get(t, f\"<ID:{t}>\")\n",
                "            print(f\"   [{i:3d}] {token_name:8s} (p1={p1}, p2={p2})\")\n",
                "        if len(pred) > 20:\n",
                "            print(f\"   ... ({len(pred) - 20} more tuples)\")\n",
                "\n",
                "        # Compile test\n",
                "        try:\n",
                "            from alphaknit.compiler import KnittingCompiler\n",
                "            compiler = KnittingCompiler()\n",
                "            tokens = [f\"{config.ID_TO_TOKEN.get(t, '<UNK>')}({p1},{p2})\" for t, p1, p2 in pred]\n",
                "            graph = compiler.compile(tokens)\n",
                "            print(f\"\\n‚úÖ Compile SUCCESS! Graph: {len(graph.nodes)} nodes.\")\n",
                "        except Exception as e:\n",
                "            print(f\"\\n‚ùå Compile failed: {e}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No trained model found. Run training first!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}